{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4daef598",
   "metadata": {},
   "source": [
    "### Q1 Explain the concept of forward propagation in a neural network\n",
    "\n",
    "\n",
    "Forward Propagation:\n",
    "\n",
    "Forward propagation is the process of passing input data through a neural network to compute the output (predictions). It is a key step in training and inference.\n",
    "\n",
    "The Input layer is fed with the different features (x1,x2,x3.......xn). These features are assigned weights and a bias for each layer. \n",
    "\n",
    "Activation Function:\n",
    "\n",
    "Apply an activation function (e.g., ReLU, Sigmoid) to the weighted sum to introduce non-linearity.\n",
    "\n",
    "Layer-by-Layer Propagation:\n",
    "\n",
    "Repeat the weighted sum and activation process for all layers in the network.\n",
    "Output Layer:\n",
    "\n",
    "The final layer outputs predictions (e.g., probabilities for classification or a value for regression).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef9274",
   "metadata": {},
   "source": [
    "### Q2 What is the purpose of the activation function in forward propagation\n",
    "\n",
    "\n",
    "The activation function in forward propagation introduces non-linearity to the neural network, enabling it to learn and model complex patterns and relationships in data.\n",
    "\n",
    "Purpose of Activation Functions:\n",
    "Non-Linearity:\n",
    "\n",
    "Without activation functions, the network would behave like a linear model, regardless of its depth, limiting its ability to solve complex problems.\n",
    "Feature Transformation:\n",
    "\n",
    "Transforms the input into a space where features are separable, making it easier for the network to classify or predict.\n",
    "Universal Approximation:\n",
    "\n",
    "Enables neural networks to approximate any continuous function, making them highly versatile.\n",
    "Gradient Flow:\n",
    "\n",
    "Helps control the flow of gradients during backpropagation, ensuring effective weight updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf080459",
   "metadata": {},
   "source": [
    "### Q3 Describe the steps involved in the backward propagation (backpropagation) algorithm\n",
    "\n",
    "\n",
    "Step 1: Forward Propagation where the output y(pred) is calculated using summation of weights and biases and then activation             function. Compute the loss function (L).\n",
    "\n",
    "Step 2: Calculate the derivative of the loss with respect to the output layer's weights and biases.\n",
    "\n",
    "Step 3: Use the chain rule to compute the gradients layer-by-layer. \n",
    "\n",
    "Step 4: Update the weights and biases using gradient descent or a similar optimization algorithm:\n",
    "\n",
    "        W(new)=W(old)-n∂L/∂W where n=learning rate.\n",
    "      \n",
    "Repeat the above given steps for each layer and keep updating all the weights.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b40683",
   "metadata": {},
   "source": [
    "### Q4 What is the purpose of the chain rule in backpropagation\n",
    "\n",
    "Purpose of the Chain Rule in Backpropagation:\n",
    "Gradient Calculation Across Layers:\n",
    "\n",
    "Neural networks consist of multiple layers, and the chain rule computes how changes in weights at one layer affect the loss.\n",
    "Handling Composite Functions:\n",
    "\n",
    "The output of each layer is a function of the outputs of the previous layers. The chain rule allows the computation of derivatives for these composite functions.\n",
    "Efficient Error Propagation:\n",
    "\n",
    "The chain rule propagates errors from the output layer back to the input layer, ensuring all parameters contribute to minimizing the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debca04c",
   "metadata": {},
   "source": [
    "### Q5 Implement the forward propagation process for a simple neural network with one hidden layer using NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7bd43f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the network: [[0.90574335]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function (Sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward propagation function\n",
    "def forward_propagation(X, weights, biases):\n",
    "    # Hidden layer computation\n",
    "    Z1 = np.dot(weights['W1'], X) + biases['b1']  # Weighted sum for hidden layer\n",
    "    A1 = sigmoid(Z1)                              # Activation for hidden layer\n",
    "    \n",
    "    # Output layer computation\n",
    "    Z2 = np.dot(weights['W2'], A1) + biases['b2'] # Weighted sum for output layer\n",
    "    A2 = sigmoid(Z2)                              # Activation for output layer (Sigmoid for binary classification)\n",
    "    \n",
    "    return A2, {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "\n",
    "# Example inputs\n",
    "np.random.seed(0)  # For reproducibility\n",
    "X = np.array([[0.5], [0.3]])  # Input features (2x1)\n",
    "weights = {\n",
    "    \"W1\": np.random.rand(3, 2),  # Weights for hidden layer (3 neurons, 2 inputs)\n",
    "    \"W2\": np.random.rand(1, 3)   # Weights for output layer (1 neuron, 3 inputs)\n",
    "}\n",
    "biases = {\n",
    "    \"b1\": np.random.rand(3, 1),  # Biases for hidden layer (3 neurons)\n",
    "    \"b2\": np.random.rand(1, 1)   # Bias for output layer (1 neuron)\n",
    "}\n",
    "\n",
    "# Perform forward propagation\n",
    "output, cache = forward_propagation(X, weights, biases)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output of the network:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2ef31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
